{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bec0e20b",
   "metadata": {},
   "source": [
    "# Jonathan Gotian, Aarya Tavshikar, Jeanie Liu, Conor Mervyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a57acad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import requests\n",
    "#from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import seaborn\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "%load_ext sql\n",
    "\n",
    "%config SqlMagic.autopandas = True\n",
    "%config SqlMagic.feedback = False\n",
    "%config SqlMagic.displaycon = False\n",
    "\n",
    "%sql duckdb:///:memory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3be22f",
   "metadata": {},
   "source": [
    "**Research Question:** What is a better indicator of win probability across Elo ratings and time constraints: Opening played or Elo rating differential, or a combination of both? If both, what is the most accurate weighting for both Opening played and Elo rating differential to maximize win probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75231449",
   "metadata": {},
   "source": [
    "# Raw Data Observations and Attributes Legend\n",
    "\n",
    "Each point, or observation, within the dataset consists of an individual chess game. The observations collected about each chess game include the following attributes:\n",
    "\n",
    "* Event: The formal naming for this instance's time control variation\n",
    "* Black: The Lichess username of the player using black pieces\n",
    "* White: The Lichess username of the player using white pieces\n",
    "* Result: The result of each game, denoted by '1-0' for White winning, '0-1' for Black winning, and '0.5-0.5' for a draw \n",
    "* UTCDate: The date in which the game was played\n",
    "* UTCTime: The time in which the game was played\n",
    "* WhiteELO: The Elo ratings for the white player\n",
    "* BlackELO: The Elo ratings for the black player\n",
    "* WhiteRatingDiff: How many points White gains/loses from the outcome of the game\n",
    "* BlackRatingDiff: How many points Black gains/loses from the outcome of the game\n",
    "* ECO: The Encyclopedia Chess Opening codes (which denotes the opening moves used by each player via a shorthand code)\n",
    "* Opening: The full name of the played opening strategy\n",
    "* Time Control: The Time Control for each game in seconds (time limits broken down into the format ‘Base Time + Increment’)\n",
    "    * Base Time is the total base time given to each player for the entire game \n",
    "    * Increment is the additional time added to each player’s total time after each of their respective moves\n",
    "* Termination: The Termination of each game (how each game was ended). The two main ones are:\n",
    "    * Normal - a player was checkmated (left with no more moves)\n",
    "    * Time forfeit - a player's base time reached 0 seconds before either player was able to complete the game\n",
    "* AN: The exact moves each player did during the game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2fc82b",
   "metadata": {},
   "source": [
    "**Data Cleaning: Importing Data**\n",
    "\n",
    "Here we imported raw data from a csv file that we downloaded from our Kaggle data source. This is stored inside the value chess_games. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7daefe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import raw data\n",
    "chess_games = pd.read_csv('chess_games.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296fb6ff",
   "metadata": {},
   "source": [
    "**Data Cleaning: Column Types**\n",
    "\n",
    "After inspecting the excel file, we realized that the UTCDate and UTCTime columns used strange formatting which would not convert properly into a dataframe. To rectify this, we coverted these columns to datetime. We also ran a test to confirm that the dtypes for other columns are expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f3ee458",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'UTCDate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/info2950/lib/python3.8/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/info2950/lib/python3.8/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/info2950/lib/python3.8/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'UTCDate'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Data Cleaning - convert date and time columns to datatime objects\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m chess_games[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTCDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mchess_games\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUTCDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      3\u001b[0m chess_games[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTCTime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(chess_games[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTCTime\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m display(chess_games\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/info2950/lib/python3.8/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/info2950/lib/python3.8/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'UTCDate'"
     ]
    }
   ],
   "source": [
    "#Data Cleaning - convert date and time columns to datatime objects\n",
    "chess_games['UTCDate'] = pd.to_datetime(chess_games['UTCDate'])\n",
    "chess_games['UTCTime'] = pd.to_datetime(chess_games['UTCTime'])\n",
    "\n",
    "display(chess_games.head())\n",
    "\n",
    "#Check to ensure each column type is correct\n",
    "print(chess_games.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96ce577",
   "metadata": {},
   "source": [
    "**Data Cleaning: Termination Column**\n",
    "\n",
    "The first column that we wanted to clean was the termination and results columns. In these columns, there were a couple of values which did not make sense in the context of the data. We also wanted to filter out terminations due to abandoned games, caused when one player suddenly quits, or rules infractions, caused when a player is found to be cheating. \n",
    "\n",
    "This cleaned data is stored in the value completed_chess_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af011fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out Termination = Abandoned, Rules infraction, filtering out TimeControl = '-'\n",
    "#checking that there are no games where result is * (meaning incomplete)\n",
    "completed_chess_games = %sql SELECT * FROM chess_games \\\n",
    "WHERE Termination NOT IN ('Abandoned', 'Rules infraction') \\\n",
    "    AND TimeControl != '-' \\\n",
    "    AND Result != '*'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0499443d",
   "metadata": {},
   "source": [
    "**Data Cleaning: Removing Initial NaNs**\n",
    "\n",
    "Here, we filtered out all the NaNs from the completed_chess_games dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a8ce41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filter out all NaN values\n",
    "completed_chess_games = completed_chess_games.dropna()\n",
    "completed_chess_games.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1355495",
   "metadata": {},
   "source": [
    "**Data Cleaning: Less Necessary Columns**\n",
    "\n",
    "At this point, we realized that certain columns were useless to us. For example, it does not matter when games were played for our research, so we dropped these columns. Because our research looks at rating differentials, we calculated this value and stored it in a new column in the dataframe (WhiteRatingDiff). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cba71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new table filtering out unneccsary columns\n",
    "#Creating a column WhiteRatingDiff for the rating differential between White and Black from white's perspective\n",
    "completed_chess_games = %sql SELECT Event, Result, WhiteElo, BlackElo, (WhiteElo - BlackElo) AS WhiteRatingDiff, ECO, Opening, TimeControl, Termination \\\n",
    "FROM completed_chess_games"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a812678a",
   "metadata": {},
   "source": [
    "**Data Cleaning: Removing Additional NaNs**\n",
    "\n",
    "Again, we had some NaNs in our dataframe under the WhiteRatingDiff column which arose due to the previous calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541f6692",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filter out all NaN values\n",
    "completed_chess_games = completed_chess_games.dropna()\n",
    "completed_chess_games"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0dfaa1",
   "metadata": {},
   "source": [
    "**Data Cleaning: Time Control**\n",
    "\n",
    "Because our cleaned dataset of completed chess games was so large (~6.2 million rows), we decided to only take the top 10% of rows. \n",
    "\n",
    "We decided to use the top 10% of rows from the completed chess games as opposed to grouping by Opening, ELO Rating Differential, Time Control, or ELO Rating and taking the top 10% of rows from the group is that grouping the data by one variable can skew the distributions of the other variables in the data and in turn, skew the analysis of the data. It would not be a true random sample like taking the top 10% of rows.  \n",
    "\n",
    "One of the final things to clean was the time control column. In chess, a time control is split into a base and an increment. The base denotes how much time you have to play the game and the increment denotes how much time is added to your remaining time per move you play. The standard notation for this is time control = base+increment. \n",
    "\n",
    "Here, we wanted to split the base and increment into their own columns to make the data easier to understand. After doing this, we joined the data from the time control rows with the data from the top 10% of rows. We then changed the units to make more sense, checked that the dtype for the new columns was correct, and saved this dataset into a new dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8179339e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Sliced distribution - taking the top 10% of rows \n",
    "chess_games_cut = completed_chess_games[completed_chess_games['Event'].index <= len(completed_chess_games)*.10]\n",
    "\n",
    "#FINAL CLEANING - spliting base and incremental into two different columns\n",
    "#split the time control into two columns - Base and Incremental\n",
    "time_control_df = chess_games_cut.TimeControl.apply(lambda x: pd.Series(str(x).split(\"+\")))\n",
    "\n",
    "time_control_df = time_control_df.rename(columns={0:\"Base\", 1:\"Increment\"})\n",
    "display(time_control_df)\n",
    "\n",
    "#Join chess games 10% cut df with time control split df together\n",
    "chess_games_tc_split = pd.concat([chess_games_cut, time_control_df], axis=1)\n",
    "print(chess_games_tc_split.dtypes)\n",
    "\n",
    "#removing first column which has row numbers only\n",
    "chess_games_tc_split = chess_games_tc_split.iloc[:,1:]\n",
    "\n",
    "#converting 'Base' to minutes in floats and 'Increment' in ints\n",
    "chess_games_tc_split['Base'] = chess_games_tc_split['Base'].astype(int) / 60\n",
    "#chess_games_tc_split['Base'] = chess_games_tc_split['Base'] / 60\n",
    "chess_games_tc_split['Increment'] = chess_games_tc_split['Increment'].astype(int)\n",
    "\n",
    "#renaming Base and Increment columns with units\n",
    "chess_games_tc_split = chess_games_tc_split.rename(columns = {'Base': 'Base (min)', 'Increment': 'Increment (sec)'})\n",
    "\n",
    "#checking for data types of each column\n",
    "print(chess_games_tc_split.dtypes)\n",
    "display(chess_games_tc_split)\n",
    "\n",
    "#saving a csv file of the cleaned data frame\n",
    "#chess_games_tc_split.to_csv('chess_games_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72710804",
   "metadata": {},
   "source": [
    "# Data Sample Distribution Testing \n",
    "**Data Testing: Opening Distributions**\n",
    "\n",
    "Because we sliced to only include the top 10% of rows, we wanted to ensure that the data was relatively similar, specifically in the distributions of openings. We did this by comparing the number of occurances of each opening before and after our slice, as well as the percentage of the total that these openings were. We then compared the difference in percentage of the density of these openings before and after our split, creating a new column Percent_Diff in a new joined table. \n",
    "\n",
    "As shown in the resulting table, some openings which were only played in games that were sliced did not show up after the slice happened. These resulted in NaN values for Cut_Count, Cut_Percent_Tot, and Percent_Diff. \n",
    "\n",
    "Given that the percent difference was so small between the sliced and original dataset, we can assume that our slice is accurate at representing our original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab589ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING DISTRIBUTIONS - OPENING\n",
    "#Opening distribution - pre slice\n",
    "#Group by opening, and count each occurance of the opening\n",
    "opening_dist = completed_chess_games.groupby(by = 'Opening')\n",
    "opening_dist = opening_dist.count().sort_values(by = 'Event', ascending=False)\n",
    "\n",
    "\n",
    "#Calculate the percent of total occurances for each opening \n",
    "opening_dist_1 = opening_dist.loc[:, ['Event']]\n",
    "opening_dist_1 = opening_dist_1.rename(columns={\"Event\":\"Count\"}) #renaming column 'Event' to 'Count'\n",
    "opening_dist_1['Percent_Tot'] = opening_dist_1['Count']/len(completed_chess_games)\n",
    "#filtering for top 0.01% of rows to get rid of openings that have minimal occurances\n",
    "opening_dist_1 = opening_dist_1[opening_dist_1['Percent_Tot'] > 0.0001]\n",
    "\n",
    "#2940 rows - no filter for > 0.01% of Total\n",
    "#958 rows - filter for > 0.01% of Total\n",
    "\n",
    "\n",
    "#Opening distribution - post slice\n",
    "#Group by opening, and count each occurance of the opening\n",
    "opening_dist_cut = chess_games_cut.groupby(by = 'Opening')\n",
    "opening_dist_cut = opening_dist_cut.count().sort_values(by = 'Event', ascending=False)\n",
    "#opening_dist_cut\n",
    "\n",
    "#Calculate the percent of total occurances for each opening \n",
    "opening_dist_cut_1 = opening_dist_cut.loc[:, ['Event']]\n",
    "opening_dist_cut_1 = opening_dist_cut_1.rename(columns={\"Event\":\"CUT_Count\"})\n",
    "opening_dist_cut_1['CUT_Percent_Tot'] = opening_dist_cut_1['CUT_Count']/len(chess_games_cut) #renaming column 'Event' to 'Count'\n",
    "opening_dist_cut_1 = opening_dist_cut_1[opening_dist_cut_1['CUT_Percent_Tot'] > 0.0001]\n",
    "#filtering for top 0.01% of rows to get rid of openings that have minimal occurances\n",
    "#opening_dist_cut_1\n",
    "\n",
    "#2623 rows - no filter for > 0.01% of Total\n",
    "#956 rows - filter for > 0.01% of Total\n",
    "\n",
    "\n",
    "#Joining pre-cut and post-cut tables\n",
    "open_joined = opening_dist_1.join(opening_dist_cut_1, on = \"Opening\")\n",
    "open_joined['Percent_Diff'] = open_joined['Percent_Tot'] - open_joined['CUT_Percent_Tot']\n",
    "open_joined\n",
    "#958 rows with NaN included\n",
    "#924 rows NaN filtered out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31ec3be",
   "metadata": {},
   "source": [
    "**Data Testing: ELO Rating Differential Distributions**\n",
    "\n",
    "To reaffirm that our split for our random sample was successful at representing the original dataset, we decided to create histograms of the rating differentials in the dataset before and after the split. \n",
    "\n",
    "These distributions look the same, so we can reaffirm that the cut was successful based on rating differential. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ed616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING DISTRIBUTIONS - RATING DIFFERENTIAL \n",
    "#TOP = Distribution of Original DF\n",
    "completed_chess_games.hist(column = 'WhiteRatingDiff', bins = 100, range = (-800, 800))\n",
    "\n",
    "#Bottom = Distribution of Cut DF\n",
    "chess_games_cut.hist(column = 'WhiteRatingDiff', bins = 100, range = (-800, 800))\n",
    "\n",
    "#Distributions look the same so good cut based on Rating Differential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37ea58b",
   "metadata": {},
   "source": [
    "**Data Testing: ELO Rating Distribution**\n",
    "\n",
    "Another way to validate if our data is properly distributed is to inspect rating distributions. Specifically, this would tell us if both datasets contain players of the same rating range. \n",
    "\n",
    "The histograms for both black and white's ELO rating looks the same between the datasets before and after slicing. Thus, we can assume that the cut was successful based on rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bcd42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING DISTRIBUTIONS - RATING  \n",
    "#TOP = Distribution of Original DF\n",
    "completed_chess_games.hist(column = ['WhiteElo', 'BlackElo'], bins = 100, range = (500,2750))\n",
    "\n",
    "#BOTTOM = Distribution of Cut DF\n",
    "chess_games_cut.hist(column = ['WhiteElo', 'BlackElo'], bins = 100, range = (500,2750))\n",
    "\n",
    "\n",
    "#Distributions look the same so good cut based on Rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e894df2",
   "metadata": {},
   "source": [
    "**Data Testing: Time Control Distributions**\n",
    "\n",
    "As we did previously, we wanted to ensure that the data was relatively similar, specifically in the distributions of time controls. We did this by comparing the number of occurances of each time control before and after our slice, as well as the percentage of the total that these controls were. We then compared the difference in percentage of the density of these controls before and after our split, creating a new column Percent_Diff in a new joined table. \n",
    "\n",
    "As shown in the resulting table, some controls which were only played in games that were sliced did not show up after the slice happened. These resulted in NaN values for Cut_Count, Cut_Percent_Tot, and Percent_Diff. \n",
    "\n",
    "Given that the percent difference was so small between the sliced and original dataset, we can assume that our slice is accurate at representing our original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd81d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING DISTRIBUTIONS - TIME CONTROL\n",
    "#Time Control distribution - pre slice\n",
    "#Group by Time Control, and count each occurance of the Control\n",
    "timecontrol_dist = completed_chess_games.groupby(by = 'TimeControl')\n",
    "timecontrol_dist = timecontrol_dist.count().sort_values(by = 'Event', ascending=False)\n",
    "\n",
    "\n",
    "#Calculate the percent of total occurances for each opening \n",
    "timecontrol_dist_1 = timecontrol_dist.loc[:, ['Event']]\n",
    "timecontrol_dist_1 = timecontrol_dist_1.rename(columns={\"Event\":\"Count\"})\n",
    "timecontrol_dist_1['Percent_Tot'] = timecontrol_dist_1['Count']/len(completed_chess_games)\n",
    "timecontrol_dist_1 = timecontrol_dist_1[timecontrol_dist_1['Percent_Tot'] > 0.0001]\n",
    "\n",
    "#840 rows - no filter for 0.01% of Total\n",
    "#189 rows - filter for 0.01% of Total\n",
    "\n",
    "\n",
    "#Opening distribution - post slice\n",
    "#Group by opening, and count each occurance of the opening\n",
    "timecontrol_dist_cut = chess_games_cut.groupby(by = 'TimeControl')\n",
    "timecontrol_dist_cut = timecontrol_dist_cut.count().sort_values(by = 'Event', ascending=False)\n",
    "timecontrol_dist_cut\n",
    "\n",
    "#Calculate the percent of total occurances for each opening \n",
    "timecontrol_dist_cut_1 = timecontrol_dist_cut.loc[:, ['Event']]\n",
    "timecontrol_dist_cut_1 = timecontrol_dist_cut_1.rename(columns={\"Event\":\"CUT_Count\"})\n",
    "timecontrol_dist_cut_1['CUT_Percent_Tot'] = timecontrol_dist_cut_1['CUT_Count']/len(chess_games_cut)\n",
    "timecontrol_dist_cut_1 = timecontrol_dist_cut_1[timecontrol_dist_cut_1['CUT_Percent_Tot'] > 0.0001]\n",
    "timecontrol_dist_cut_1\n",
    "\n",
    "#534 rows - no filter for > 0.01% of Total\n",
    "#182 rows - filter for > 0.01% of Total\n",
    "\n",
    "\n",
    "#Joining pre-cut and post-cut tables\n",
    "tc_joined = timecontrol_dist_1.join(timecontrol_dist_cut_1, on = \"TimeControl\")\n",
    "tc_joined['Percent_Diff'] = tc_joined['Percent_Tot'] - tc_joined['CUT_Percent_Tot']\n",
    "tc_joined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c364664",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "\n",
    "### Motivation\n",
    "\n",
    "**For what purpose was the dataset created?**\n",
    "\n",
    "This dataset was created to analyze our research question which looks at if elo rating differential or time control is a better indicator of win probability. \n",
    "\n",
    "**Who funded the creation of the dataset?**\n",
    "\n",
    "The funding for the original dataset is unknown at this time, but all data was published by lichess.org. Lichess is a free, open-source, non-profit organization. \n",
    "\n",
    "### Composition:\n",
    "\n",
    "**What do the instances that comprise the dataset represent?**\n",
    "\n",
    "Instances of the data represent individual games played on the lichess platform. \n",
    "\n",
    "**How many instances are there in total?**\n",
    "\n",
    "The data includes approximately 620,000 individual games.\n",
    "\n",
    "**Does the dataset contain all possible instances or is it a sample of instances from a larger set?**\n",
    "\n",
    "The dataset does not include all possible instances, as we chose to only include 10% of the instances from the larger raw dataset extracted from Kaggle. Additionally, because the data was collected over a brief period of time using only one online platform, it is not immediately clear if it is representative of all chess games played. \n",
    "\n",
    "We conducted statistical analyses, checking if the difference in densities for openings played and time controls was different between our dataset and the larger dataset. These analyses found no noticeable differences, ensuring that our dataset was representative of the larger set. \n",
    "\n",
    "**What are the observations (rows) and the attributes (columns)?**\n",
    "\n",
    "***Columns:***\n",
    "\n",
    "* Result: The result of each game, denoted by '1-0' for White winning, '0-1' for Black winning, and '0.5-0.5' for a draw. \n",
    "WhiteElo: The ELO rating for the player using white pieces\n",
    "\n",
    "* BlackElo: The ELO rating for the player using black pieces\n",
    "\n",
    "* WhiteRatingDiff: The ELO rating differential from the perspective of the player using white pieces. \n",
    "\n",
    "* ECO: The Encyclopedia Chess Opening codes which denotes the opening moves used by each player via a shorthand code. \n",
    "\n",
    "* Opening: The full name of each opening strategy, a pattern of moves a player uses to open the game. \n",
    "\n",
    "* Time Control: The time limit for each game in seconds (time limits broken down into the format ‘Base Time + Increment’)\n",
    "\n",
    "* Termination: The Termination of each game (how each game was ended). The two main ones are:\n",
    "    * Normal - a player was checkmated (left with no more moves)\n",
    "    * Time forfeit - a player's base time reached 0 seconds before either player was able to complete the game\n",
    "\n",
    "* Base Time is the total base time given to each player for the entire game\n",
    "\n",
    "* Increment is the additional time added to each player’s total time after each of their respective moves\n",
    "\n",
    "***Rows:***\n",
    "\n",
    "Each row represents an individual game\n",
    "\n",
    "**Is any information missing from individual instances?**\n",
    "No. Data was cleaned such that each instance is complete. \n",
    "\n",
    "**Is the dataset self-contained or does it link to or otherwise rely on external resources?**\n",
    "Yes, the dataset is self-contained. Although the data was extracted from an online chess platform, our collection of data will remain static. \n",
    "\n",
    "### Collection Process\n",
    "\n",
    "**How was the data associated with each instance acquired?**\n",
    "The data was collected from lichess, but it is not directly clear where on lichess this data came from. \n",
    "\n",
    "**Over what timeframe was the data collected?**\n",
    "Per the data description on Kaggle, the data was collected over all chess games played on Lichess in July of 2016. \n",
    "\n",
    "**Did you collect data from individuals in question directly or obtain it via third parties or other sources?**\n",
    "Our data does collect information about people, as each chess player is associated with an actual person. The data was not collected from these individuals directly, but was obtained from Lichess, the platform where they played chess. \n",
    "\n",
    "**Were the individuals in question notified about the data collection, and if so, what purpose did they expect the data to be used for?**\n",
    "Yes, Lichess does explicitly notify individuals of data collection for statistical purposes, as outlined in their privacy policy:\n",
    "\n",
    "“In order to maintain statistics relating to the games that took place on the lichess.org site and in particular to allow you to know your history of the games played as well as your ranking, we will keep your username and other non-personal information, such as party metadata.\n",
    "This further processing is compatible with the initial collection of your username, justified by the execution of the contract in accordance with Article 6.1.b of the GDPR as indicated in point 2 above, in accordance with recital 50 and Articles 5 and 89 of the GDPR.”\n",
    "    \n",
    "As indicated in the above privacy policy, individuals expected their data to be used for statistical purposes, which falls within the bounds of this project. \n",
    "\n",
    "### Preprocessing/cleaning/labeling\n",
    "\n",
    "**Was any preprocessing/cleaning/labeling of the data done?**\n",
    "Yes, the data was taken from a relatively raw format and preprocessed to be analyzable. This involved a couple of steps. First, we shrunk down the data into 10% of its original size, in order to make our analysis more efficient. We also worked to clean some columns, removing instances where columns had unexpected or invalid values, such as termination due to cheating or abandonment. We compared our preprocessed data to the raw dataset, ensuring that our preprocessed data was not noticeably different. \n",
    "\n",
    "**Was the “raw” data saved in addition to the preprocessed/cleaned/labeled data?**\n",
    "Yes. A link to this data is below:\n",
    "https://cornell.box.com/s/tey4tig1xn8ngpml1x0c866nvkz65jkj\n",
    "\n",
    "### Uses\n",
    "**Has the dataset been used for any tasks already?**\n",
    "Yes. The dataset has been used to quantify high-level summary statistics and draw charts including histograms, density charts, and bar charts. These summary diagrams have given us some insight into the dataset. \n",
    "**What (other) tasks could the dataset be used for?**\n",
    "The dataset will be used to run more complicated statistical testing, including a hypothesis test, to better quantify our research question. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961e9049",
   "metadata": {},
   "source": [
    "# Data Limitations\n",
    "\n",
    "There are a couple of obvious limitations to our raw and analysis-ready dataset. The first limitation comes from the source of our data. Specifically, because this is data collected for all games played on the Lichess platform during July 2016, it cannot be said to be representative of all games played on Lichess, or even all games played on chess platforms on the internet. More specifically, there are other massively popular online chess websites (chess.com or chess24.com, etc.) which also have millions of games played every month. \n",
    "\n",
    "This is further amplified by the fact that a large number of chess games aren’t played online, and no data exists for them. Some data exists for in-person chess games, but these are usually limited to tournaments, and don’t usually account for amateur players or other over-the-board games. \n",
    "\n",
    "Another limitation of our data relates to our Termination values. There were four unique values here, ‘Abandoned’, ‘Time forfeit’, ‘Normal’, and ‘Rules Infraction’. Rules Infraction was used to denote games where one player was found to be cheating, based on if they were playing suspiciously above a human level. One limitation of this data is that players could have cheated on games but not been caught by the Lichess platform. This is a relatively common occurrence, and can add noise to data on game results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f814ea9d",
   "metadata": {},
   "source": [
    "# Summary and Descriptive Stats\n",
    "\n",
    "Here, we changed the name of the dataframe to chess_games for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ade9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chess_games = chess_games_tc_split\n",
    "print(display(chess_games))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760386ef",
   "metadata": {},
   "source": [
    "**Computing Win Rates of Each Opening**\n",
    "\n",
    "As part of our research question is about predicting win probability with opening played, we computed the win rate of each opening. We first tallied the number of games played for each opening in a data frame and then tallied the number of games won for each opening in another data frame.  We then joined the 2 data frames by opening.\n",
    "\n",
    "Since there were openings with no games won, joining the two data frames resulted in NaNs in the Games_Won column. We thus dropped all NaNs.\n",
    "\n",
    "Because we did not want openings that were played infrequently to seemingly have high win rates, we dropped all openings that had fewer than 50 games played.\n",
    "\n",
    "Finally, we computed the win rate by dividing Games_Won by Games_Played and dropped the Games_Won and Games_Played columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a98eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding win rate column for each opening\n",
    "\n",
    "#all openings with corresponding number of games played\n",
    "%sql opening_games_played << SELECT Opening, COUNT(ECO) As Games_Played \\\n",
    "FROM chess_games GROUP BY Opening ORDER BY Games_Played DESC\n",
    "\n",
    "print(opening_games_played)\n",
    "\n",
    "#all openings with corresponding number of games won\n",
    "%sql opening_games_won << SELECT Opening, COUNT(ECO) AS Games_Won \\\n",
    "FROM chess_games WHERE Result = '1-0' GROUP BY Opening ORDER BY Games_Won DESC\n",
    "\n",
    "print(opening_games_won)\n",
    "\n",
    "#joining games played and games won by opening\n",
    "%sql opening_games << SELECT opening_games_played.Opening, Games_Played, Games_Won \\\n",
    "FROM opening_games_played LEFT JOIN opening_games_won \\\n",
    "ON opening_games_played.Opening = opening_games_won.Opening ORDER BY Games_Played DESC\n",
    "\n",
    "#drop NaNs --> NaNs in Games_Won column indicate that opening has 0 games won\n",
    "opening_games.dropna()\n",
    "\n",
    "#filter for openings that have at least 50 games played\n",
    "opening_games = opening_games[opening_games['Games_Played'] >= 50]\n",
    "\n",
    "#compute win rate column and sort by win rate\n",
    "opening_games['Win_Rate'] = opening_games['Games_Won'] / opening_games['Games_Played']\n",
    "opening_win_rate = opening_games.sort_values(by = ['Win_Rate'], ascending = False)\n",
    "\n",
    "#drop columns Games_Played and Games_Won; rename Opening to Opening_Type for later merging\n",
    "opening_win_rate = opening_win_rate.drop(columns = ['Games_Played', 'Games_Won'])\n",
    "opening_win_rate = opening_win_rate.rename(columns={'Opening': 'Opening_Type'})\n",
    "\n",
    "display(opening_win_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50db7922",
   "metadata": {},
   "source": [
    "**Appending win probability to the list of all chess games**\n",
    "\n",
    "Using the data frame with each opening and its corresponding win rate, we appended the win probability to each chess game played according to the opening played. Since we had fewer openings with win rates after filtering to at least 50 games played,  we used an inner join to avoid NaNs for chess games that had openings that were not in the list of openings with win rates data frame. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303bb40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding win rate to individual games data frame\n",
    "%sql chess_games_rate << SELECT * FROM chess_games INNER JOIN opening_win_rate \\\n",
    "ON chess_games.Opening = opening_win_rate.Opening_Type\n",
    "\n",
    "chess_games_rate = chess_games_rate.drop(columns = ['Opening_Type'])\n",
    "display(chess_games_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89938002",
   "metadata": {},
   "source": [
    "**Descriptive Statistics**\n",
    "\n",
    "For our next steps, we would like to see whether opening played or rating differential is a better predictor of win probability at different Elo ratings and time constraints. Thus, performing descriptive statistics and seeing the distribution of Elo ratings and time constraints would help us divide them into multiple ranges.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a54fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#statistical summary of the chess games along with win rates\n",
    "display(chess_games_rate.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46836d61",
   "metadata": {},
   "source": [
    "**Most Popular Openings and Best Openings by Win Rate**\n",
    "\n",
    "We graphed the top 10 played openings and found that van't kruijs opening is the most popular opening by a huge margin with over 12,000 games played. The other 9 popular openings have at least 7000 games played to over 11,000 games played. \n",
    "\n",
    "We also graphed the top 10 openings by their win rate. These openings have win rates in the range of 0.7 and 0.8. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b92a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 popular openings & top 10 openings by win rate\n",
    "%sql top_ten_popular_openings << SELECT Opening, COUNT(ECO) As Games_Played \\\n",
    "FROM chess_games_rate GROUP BY Opening ORDER BY Games_Played DESC LIMIT 10\n",
    "%sql top_ten_openings_rate << SELECT * FROM opening_games ORDER BY Win_Rate DESC LIMIT 10\n",
    "\n",
    "#plotting the top 10 popular openings & openings by win rate\n",
    "seaborn.barplot(data = top_ten_popular_openings, x = 'Games_Played', y = 'Opening').set(title = 'Top 10 Popular Openings')\n",
    "pyplot.show()\n",
    "seaborn.barplot(data = top_ten_openings_rate, x = 'Win_Rate', y = 'Opening').set(title = 'Top 10 Openings by Win Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f467cc",
   "metadata": {},
   "source": [
    "**Worse Openings by Win Rate**\n",
    "\n",
    "Since we limited openings to those having at least 50 games played, graphing the least 10 popular openings was not meaningful as they all just have 50 games played. Hence, we graphed the bottom 10 openings by win rate only. We found that the worse openings have win rates that fall between 0.28 and 0.34. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95868c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bottom 10 openings by win rate\n",
    "%sql bottom_ten_openings_rate << SELECT * FROM opening_games ORDER BY Win_Rate LIMIT 10\n",
    "%sql bottom_ten_openings_rate << SELECT * FROM bottom_ten_openings_rate ORDER BY Win_Rate DESC\n",
    "\n",
    "#plotting the bottom 10 openings by win rate\n",
    "seaborn.barplot(data = bottom_ten_openings_rate, x = 'Win_Rate', y = 'Opening').set(title = 'Bottom 10 Openings by Win Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb92844",
   "metadata": {},
   "source": [
    "**Plotting histograms**\n",
    "\n",
    "We plotted histograms on rating differential, time control variables, termination type, and win rate so that we could see their distributions.\n",
    "\n",
    "We found that the rating differential is normally distributed with a mean of 0. This means the majority of chess games played had players that were at similar skill levels. \n",
    "\n",
    "We further found that most chess games were played at 1, 3, 5, 10 minute bases. This is not surprising since these are the preset time bases in lichess. \n",
    "\n",
    "Most games were played with 0 increment. \n",
    "\n",
    "Around 2/3 of the games had normal terminations. 1/3 had time forfeit terminations, meaning they ran out of time. \n",
    "\n",
    "Lastly, win rates ranged from a little bit less than 0.3 to a little bit less than 0.8, but most had win rates of 0.45 to 0.55.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66845b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#histograms to help us see distributions of rating differential, time control variables, termination type, and win rate\n",
    "seaborn.histplot(data = chess_games_rate, x = 'WhiteRatingDiff', binrange = (-800,800))\n",
    "pyplot.show()\n",
    "seaborn.histplot(data = chess_games_rate, x = 'Base (min)', binrange = (0,25), binwidth = 1)\n",
    "pyplot.show()\n",
    "seaborn.histplot(data = chess_games_rate, x = 'Increment (sec)', binrange = (0,10), binwidth = 1)\n",
    "pyplot.show()\n",
    "seaborn.histplot(data = chess_games_rate, x = 'Termination')\n",
    "pyplot.show()\n",
    "seaborn.histplot(data = chess_games_rate, x = 'Win_Rate', kde = True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c953cd",
   "metadata": {},
   "source": [
    "**Plotting Scatterplots**\n",
    "\n",
    "We plotted rating differential against win rates to see if there is a relationship between the two. The two variables do not seem to have an apparent relationship, but further analysis is needed. \n",
    "\n",
    "We also found that regardless of the base, all the win rates are centered around 0.5. Some bases like those in the range of 0 to 30 min, 60 min, 90 min, and 180 min seem to have greater spread of win rates, but this is most likely due to having more games that are played at these preset bases. Similarly, win rates are centered around 0.5 at different increments. Increments of 0 - 30 sec, 60 sec, 90 sec, and 180 sec have wide spread of win rates. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4990234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatterplots to see relationship between rating differential and win rate vs. base time control and win rate\n",
    "seaborn.scatterplot(data = chess_games_rate, x = 'WhiteRatingDiff', y = 'Win_Rate')\n",
    "pyplot.show()\n",
    "seaborn.scatterplot(data = chess_games_rate, x = 'Base (min)', y = 'Win_Rate')\n",
    "pyplot.show()\n",
    "seaborn.scatterplot(data = chess_games_rate, x = 'Increment (sec)', y = 'Win_Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2143a47c",
   "metadata": {},
   "source": [
    "**Correlation between rating differential and win rate**\n",
    "\n",
    "In our scatterplot, we found no apparent relationship between rating differential and win rate. Thus, we computed the correlation between the two.\n",
    "The correlation is 0.16, which is a rather weak positive linear relationship. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ad9511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation between Elo rating differential and win rate\n",
    "display(chess_games_rate[['WhiteRatingDiff','Win_Rate']].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03242737",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. Why shouldn't we try to convert opening win probability into a logarithmic function?\n",
    "\n",
    "2. What's the best way to compare the discrete variable of opening to the continuos variable of rating differential in their effect on win probability?\n",
    "\n",
    "3. How do we calculate which weighting of the two variables is ideal for determining win probability?\n",
    "\n",
    "4. How much background about chess should we include in our final summary?\n",
    "\n",
    "5. Is our reasoning for cutting openings to the top 10% sufficient in making sure that the cut is representative of the sample dataset?\n",
    "\n",
    "6. How do we decrease run times to still provide a meaningful statistical analysis with a large amount of data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f9d09",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "We have two major next steps as part of our analysis. First, because we want to run statistics on the opening, which is a discrete variable, we wanted some way to quantify their impact on win probability. Our immediate solution is to use dummy variables to represent and analyze these openings. We also want to calculate the specific weighting of ELO differential and time control which can predict win probability, which will require special calculations. We considered multiplying an opening’s win probability by 1 or -1 based on if the opening is better/worse than the average opening in terms of win probability. We wanted to graph this logarithmically to see which openings were better/worse on average compared to others, attempting to quantify the discrete opening variable. We decided not to do this upon advice from the teaching staff. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8acfc4b7cd02bd99ad0ce591c9a997513fc6401f38746a12b57fee6e68bbb014"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
